---
layout: default
title: Keynotes
image:
  card: 2025/expressiveCard.jpg
year: 2025
---

<div class="col-12 col-sm-12 col-lg-12">

      <a name="kn1"></a>
    	<h3><i>Building the Big One: A Studio-Size Light Stage</i></h3>

      <p><b>Paul Debevec</b><br/>Chief Research Officer, <i>Netflix’s Eyeline Studios</i></p>

	  <div class="row">

		<figure class="col-3 col-sm-3 col-lg-3 top0">
		  <img src="/img/2025/Paul.jpg" class="img-responsive img-thumbnail">
		</figure>

		  <p>Abstract:<br/>Compositing real actors into virtual environments has been a challenge in visual effects since the early days of motion pictures. Techniques such as rear projection, front projection, blue screen, and sodium vapor matting have provided innovative solutions for integrating actors seamlessly into virtual backgrounds. However, achieving realistic lighting has remained a critical hurdle.<br>

		  The development of light stage systems aimed to solve this issue by ensuring actors are illuminated in a way that matches their environments, leading to fully believable composites. After numerous prototypes, the studio-sized light stage is now ready. This talk will provide an overview of its development and explore the new production techniques it enables, including lighting reproduction, relighting, and large-scale volumetric capture.</p>

		  <p>Biography:<br/>Paul Debevec received his degrees in Computer Engineering and Mathematics from the University of Michigan in 1992 and earned a Ph.D. in Computer Science from the University of California, Berkeley in 1996. He is currently the Chief Research Officer at Netflix’s Eyeline Studios, an Adjunct Research Professor at the University of Southern California, and a Governor of the Visual Effects Branch of the Academy of Motion Picture Arts and Sciences.
		  His contributions to visual effects and virtual production have been featured in films such as The Matrix, Avatar, and Gravity. His work has been recognized with two Academy Awards, the SMPTE Progress Medal, and a Lifetime Achievement Emmy Award.
		  More information: <a href="http://www.debevec.org">www.debevec.org</a> </p>

	  </div>

</div>


<div class="col-12 col-sm-12 col-lg-12">

      <a name="kn1"></a>
      <h3><i>Sketching the Future: Democratising Creative Control via Sketching</i></h3>

      <p><b>Prof. Yi-Zhe Song</b><br/>Co-Director, <i>Surrey's People-Centred AI Institute</i></p>

	  <div class="row">

		<figure class="col-3 col-sm-3 col-lg-3 top0">
		  <img src="/img/2025/people-Prof. Yi-Zhe Song.050.webp" class="img-responsive img-thumbnail">
		</figure>

		 <p>Abstract:<br/>The journey of human-AI creative collaboration has been fundamentally transformed by our ability to communicate visual intent. This keynote traces the evolution of sketch-based capabilities as a democratising force in AI-powered creative systems, from early recognition challenges to today's generative frontiers.<br>

Beginning with the breakthrough of the first deep learning system to surpass human performance in sketch recognition, I will discuss how understanding the unique properties of sketches established theoretical frameworks that now underpin modern visual control mechanisms. In particular, I will examine how the inherent abstraction in sketching correlates with semantic understanding, making it an ideal modality for human-AI communication across expertise levels.<br>

The talk will highlight key milestones in this evolution: from developing fine-grained visual understanding through sketch-based image retrieval systems to creating multimodal frameworks that combine sketches with text, photos, and 3D representations, where complementary modalities achieve precision exceeding either alone. I will finish by showcasing how the lab intends to further the journey of democratising AI through recent work on DemoFusion and NitroFusion that directly addresses not only control but also accessibility.<br>

Looking forward, I will outline a vision for truly accessible creative AI systems where sketching serves as an intuitive control interface, enabling diverse users across technical backgrounds to harness powerful AI capabilities.</p>

		  <p>Biography:<br/>Yi-Zhe Song is Professor of Computer Vision and Machine Learning at the Centre for Vision Speech and Signal Processing (CVSSP) and co-director of the Surrey People-Centred AI Institute. As founder and leader of the SketchX Lab (est. 2012), he has driven groundbreaking research in sketch understanding, including the first deep neural network to surpass human performance in sketch recognition (BMVC 2015 Best Paper Award). His work spans fine-grained sketch-based image retrieval, domain generalisation, and bridging sketch with mainstream computer vision, with recent contributions in sketch-based object recognition earning a Best Paper nomination at CVPR 2023. He serves as Associate Editor for IEEE TPAMI and IJCV and has been Area Chair for ECCV, CVPR, and ICCV. Prof. Song established and directs Surrey's MSc in AI programme, following a similar initiative he created at Queen Mary University of London.</p>

	  </div>

</div><!--/span-->
